% Authors: Emily Gosti, Yannan Tuo, Sukrit Arora, Lydia Lee
% Emails: egosti@berkeley.edu, ytuo@berkeley.edu, lydia.lee@berkeley
\qns{Gram-Schmidt Practice}

\meta{
    Prereqs: Knowledge of bases}

\ans{
    Description: Describe the Gram-Schmidt process and what it does, and how to perform it. The objective of the Gram-Schmidt process is to take a set of linearly independent vectors that are not necessarily orthogonal, and create a set of vectors that span the same space and are orthogonal and of unit length. The motivation behind this is that it is often easier to work with vectors that are orthogonal and of unit length.}

\begin{enumerate}
\qitem\label{def_orthonormal}{
    What does it mean for a set of vectors to be orthonormal?}
    
\meta{
    Some linear algebra textbooks use ``orthogonal'' to describe orthonormal sets.}

\ans{
    Orthonormal is a portmanteau of ``orthogonal'' and ``normal;'' a set of vectors is orthonormal if (and only if)
    \begin{itemize}
        \item all vectors are mutually orthogonal i.e. the angle between different vectors is 90$^\circ$) and
        \item each vector is unit length
    \end{itemize}
    The set of $N$ vectors $\{\vec{v_1}, \cdots, \vec{v_N}\}$ is orthonormal if (and only if, by definition)
    $$v_i^Tv_j = \begin{cases}
                    1 &, i=j\\
                    0 &, i\neq j
                \end{cases}$$}


\qitem\label{inversion_orthonormal}{
    In standard least squares problems, we have to invert $A^TA$. However, inversions are computationally intensive (not to mention a pain to calculate by hand), and ideally we'd avoid them as much as possible; suppose $A$ is an orthonormal matrix. Does this make solving a least squares problem easier?}

\ans{
    Yes. If $A$ is orthonormal, then $A^TA = I$:
    \begin{align*}
        A^TA &= \begin{bmatrix}
            \horzbar & \vec{a_1}^T & \horzbar\\
            & \vdots &\\
            \horzbar & \vec{a_n}^T & \horzbar\\
        \end{bmatrix}
        \begin{bmatrix}
            \vertbar & & \vertbar\\
            \vec{a_1} & \cdots & \vec{a_n}\\
            \vertbar & & \vertbar\\
        \end{bmatrix}\\
            &= \begin{bmatrix}
                    \vec{a_1}^T\vec{a_1} & \cdots & \vec{a_1}^T\vec{a_n}\\
                    \vdots & \ddots & \vdots\\
                    \vec{a_n}^T\vec{a_1} & \cdots & \vec{a_n}^T\vec{a_n}
                \end{bmatrix}\\
            &= I_n
    \end{align*}
    and the least squares solution $(A^TA)^{-1}A^T\vec{b}$ becomes a simple projection: $A^T \vec{b}$. This saves us a lot computationally, since computing an inverse is fairly intensive.}

\qitem\label{span_requirement}{
    Suppose we replace $A$ with an orthonormal matrix $M$. What must be true of this matrix in order for it to give us the same least squares result?}

\ans{
    $M$'s columns must span the same space as $A$, because least squares is equivalent to projecting $\vec{b}$ onto the span (column space) of $A$.}

\qitem\label{span_equivalence}{
    Explain why span($\{\vec{v_1}$, $\vec{v_2}\}$) is equivalent to span($\{\vec{v_1}$, $\vec{v_2} - \alpha\vec{v_1}\}, \alpha \in \mathbb{R}$).}

\meta{
    The idea is to give intuition as to why Gram-Schmidt gives us a set of vectors that spans the same space.}

\ans{
    Showing that two sets $S_1$ and $S_2$ are equal has two parts: $S_1 \subseteq S_2$ \textbf{and} $S_2 \subseteq S_1$. Both of these must be shown to prove equivalence!

    The span of $\{\vec{v_1}$, $\vec{v_2}\}$ is all linear combinations of $\vec{v_1}$ and $\vec{v_2}$:
    $$\left\{\beta_1 \vec{v_1} + \beta_2 \vec{v_2} \,\forall\, \beta_1, \beta_2\in\mathbb{R}\right\}$$
    Writing out span($\vec{v_1}$, $\vec{v_2}-\alpha \vec{v_1}$) and rearranging it a bit:
    \begin{align*}
        \{\beta_3\vec{v_1} + \beta_4(\vec{v_2}-\alpha)\vec{v_1})\,&\forall\, \beta_3, \beta_4\in\mathbb{R}\}\\
        \{(\beta_3-\beta_4\alpha)\vec{v_1}, \beta_4\vec{v_2}\,&\forall\, \beta_3, \beta_4\in\mathbb{R}\}
    \end{align*}
    If we take some arbitrary $\vec{u_1}$ such that $\vec{u_1}\in\text{span}(\vec{v_1}, \vec{v_2})$, it's also in $\text{span}(\vec{v_1}$, $\vec{v_2}-\alpha \vec{v_1})$, so span($\{\vec{v_1}$, $\vec{v_2}\}$) $\in $span($\vec{v_1}$, $\vec{v_2}-\alpha \vec{v_1}$).

    Going the other way, we consider some arbitrary $\vec{u_2}$ such that $\vec{u_2}\in\text{span}(\vec{v_1}, \vec{v_2}-\alpha \vec{v_1})$. Looking at our rearrangement of the second span, we see that $u_2$ must also be in $\text{span}(\vec{v_1}, \vec{v_2})$}

\qitem\label{gs_normalize}{
    Consider now you're given a set of vectors $\{\vec{v_1}, \cdots ,\vec{v_N}\}$, and we want to find an orthonormal set of vectors with the same span as the original set. Find a vector $\vec{w_1}$ such that
    \begin{itemize}
        \item span($\vec{w_1}$) = span($\vec{v_1}$)
        \item $\norm{\vec{w_1}}_2 = 1$
    \end{itemize}}

\meta{
    Step 1: Pick an arbitrary vector in the set and normalize it; it needn't be the first vector in the set.}

\ans{
    $$\vec{w_1} = \frac{\vec{v_1}}{\norm{v_1}}$$}

\qitem\label{gs_project_indep}{
    Now suppose that your $\vec{w_1}$ from part \ref{gs_normalize} and $\vec{v_2}$ are linearly independent. How can we remove the $\vec{w_1}$ ``component'' of $\vec{v_2}$ from $\vec{v_2}$?}

\ans{
    To remove the $\vec{w_1}$ component from $\vec{v_2}$, we subtract the projection of $\vec{v_2}$ on the span of $\vec{w_1}$ from $\vec{v_2}$
    \begin{align*}
        \vec{u_2} = \vec{v_2} - \text{proj}_{\text{span}(\vec{w_1})}(\vec{v_2})
    \end{align*}
   The projection of $\vec{b}$ onto $\vec{a}$ can be considered the $\vec{a}$ component of $\vec{b}$.Recall that the formula for projection of a vector $\vec{u}$ onto $\vec{v}$ can be expressed as proj$_v$u = $\frac{u \cdot v}{\norm{v}^2} v$, where the $\norm{v}^2$ serves to maintain the magnitude of the original vector by multiplying a normalized version of $\vec{v}$. Hence, if we are normalizing the entire vector later regardless, we can ignore this and consider the projection to just be $(\vec{u} \cdot \vec{v}) \vec{v}$. We want to subtract this resulting projection from $\vec{b}$.
}

\qitem\label{gs_project_dep}{
    What would happen if $\vec{w_1}$ and $\vec{v_2}$ were linearly dependent and we performed the same operation as in \ref{gs_project_indep}?}
    
\qitem{
Using your answer from part d), how can we generate a second vector for our orthonormal set? Hint: span($\vec{v_1}$, $\vec{v_2}$) is equivalent to the subspace span($\vec{v_1}$, $\vec{v_2} - \alpha\vec{v_1}$).
}

\meta{\\
Step 2 of Gram-Schmidt: to find the orthonormalized second vector in the new orthonormal set, calculate the projection of the original second vector onto the orthonormalized first vector, and subtract this value from the original second vector. Normalize the vector.
}


\ans{
Starting with the second vector in our original set, we want to remove any 'trace' of $w_1$ from the second vector so that the resulting vectors are mutually orthogonal, but still span the same space.
\\Let us call the resulting vector $\vec{p_2}$. Then, $\vec{p_2} = \vec{v_2} - (\vec{v_2}^T\vec{w_1}){\vec{w_1}}$. (Our $\alpha$ is thus the projection of $v_2$ onto $w_1$). We want to normalize this vector before we add it to our new set, so $$\vec{w_2} = \frac{p_2}{\norm{p_2}}$$

}

\qitem{
For every subsequent vector that we add to our orthonormal set, we want to ensure that it is orthogonal to the rest of the $\vec{w}$'s that we have already added to our new set. Therefore, we must subtract the projection of the original vector onto each of the $\vec{w}$'s from its original value, similar to what we did in the last step. Derive an expression for the calculating the third vector in the orthonormal set.
}

\meta{\\
Step 3 of Gram-Schmidt: For every subsequent vector, repeat the same process, but subtract the projection of the current vector onto every other orthnormalized vector from the current vector. 
}

\ans{
Let us call the non-normalized, orthogonal vector $\vec{p_3}$. Then $\vec{p_3} = \vec{v_3} - (\vec{v_3}^T\vec{w_2}){\vec{w_2}} -  (\vec{v_3}^T\vec{w_1}){\vec{w_1}}$, where $(\vec{v_3}^T\vec{w_2}){\vec{w_2}}$ is the projection of the original third vector $\vec{v_3}$ onto the second orthonormal vector $\vec{w_2}$, and $(\vec{v_3}^T\vec{w_1}){\vec{w_1}}$ is the projection of the original third vector onto the first orthonormal vector $\vec{w_1}$ of our new set. Again, we want to normalize the resulting vector, so $$\vec{w_3} = \frac{p_3}{\norm{p_3}}$$
}


%Alternatively, example walk through with three vectors
\qitem{
Perform Gram-Schmidt on the following vectors to create an orthonormal basis that spans the same space:
\[\vec{v_1} = \begin{bmatrix}1\\-1\\1\end{bmatrix}
\vec{v_2} = \begin{bmatrix}1\\0\\1\end{bmatrix}
\vec{v_3} = \begin{bmatrix}1\\1\\2\end{bmatrix} \]
}

\ans{
For the first step, we just take the first vector in the original set, and set it as the first vector in our orthonormal set:
\[w_1 = \frac{v_1}{\norm{v_1}} = \frac{1}{\sqrt{3}}\begin{bmatrix}1\\-1\\1\end{bmatrix} = \begin{bmatrix}\frac{1}{\sqrt{3}}\\-\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{3}}\end{bmatrix} \]

Next, let's find $p_2$, the un-normalized second vector for our orthonormal set, using $v_2$:
\[p_2 = v_2 - (v_2^{\top}w_1)w_1 = \begin{bmatrix}1\\0\\1\end{bmatrix} - (\begin{bmatrix}1 & 0 & 1\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt{3}}\\-\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{3}}\end{bmatrix})\begin{bmatrix}\frac{1}{\sqrt{3}}\\-\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{3}}\end{bmatrix} = \begin{bmatrix}\frac{1}{3}\\\frac{2}{3}\\\frac{1}{3}\end{bmatrix} \]

If we want to add this vector to our orthonormal set, we have to normalize it:
\[w_2 = \sqrt{\frac{3}{2}}\begin{bmatrix}\frac{1}{3}\\\frac{2}{3}\\\frac{1}{3}\end{bmatrix} = \begin{bmatrix}\frac{\sqrt{6}}{6}\\\frac{\sqrt{6}}{3}\\\frac{\sqrt{6}}{6}\end{bmatrix}
\]

Finally, we repeat the same process to find $p_3$ and $w_3$.
\[p_3 = v_3 - (v_3^T w_1)w_1 - (v_3^T w_2)w_2 = \begin{bmatrix}1\\1\\2\end{bmatrix} - (\begin{bmatrix}1 & 1 & 2\end{bmatrix}\begin{bmatrix}\frac{1}{3}\\\frac{2}{3}\\\frac{1}{3}\end{bmatrix})\begin{bmatrix}\frac{1}{3}\\\frac{2}{3}\\\frac{1}{3}\end{bmatrix} - (\begin{bmatrix}1 & 1 & 2\end{bmatrix}\begin{bmatrix}\frac{\sqrt{6}}{6}\\\frac{\sqrt{6}}{3}\\\frac{\sqrt{6}}{6}\end{bmatrix})\begin{bmatrix}\frac{\sqrt{6}}{6}\\\frac{\sqrt{6}}{3}\\\frac{\sqrt{6}}{6}\end{bmatrix} = \begin{bmatrix}\frac{-1}{2}\\0\\\frac{1}{2}\end{bmatrix}\]
\[w_3 = \sqrt{2}\begin{bmatrix}\frac{-1}{2}\\0\\\frac{1}{2}\end{bmatrix} = \begin{bmatrix}\frac{-\sqrt{2}}{2}\\0\\\frac{\sqrt{2}}{2}\end{bmatrix}\]
}


    
\end{enumerate}